---
title: "Gaussian Process Models"
author: "Sean Martin"
date: "September 16, 2016"
output: ioslides_presentation
runtime: shiny
---

```{r,echo=FALSE,eval=TRUE}
set.seed(12345)
library(ggplot2)
library(MASS)
library(reshape2)

SE <- function(Xi,Xj, l=1, s=1) (s^2)*exp(-0.5 * (Xi - Xj)^2 / l ^ 2)
cov <- function(X, Y, l=1, s=1) outer(X, Y, SE, l, s)

# not-very efficient plotting function:

plotGP <- function(xRange,obs=c(),l,sigma,noise=0,samples=TRUE){
  x_predict <- seq(xRange[1],xRange[2],length.out=100)
  if(is.null(obs)){
    COV <- cov(x_predict,x_predict,l,sigma)
    values <- mvrnorm(3, rep(0, length=length(x_predict)), COV)
    dat <- data.frame(x=x_predict, t(values))
    dat <- melt(dat, id="x")
    
    ggplot(dat,aes(x=x,y=value)) +
      geom_rect(xmin=-Inf, xmax=Inf, ymin=-2*sigma, ymax=2*sigma, fill="grey80") +
      geom_line(aes(group=variable)) +   theme_bw() +
      ylab("output, f(x)") + xlab("input, x") +
      coord_cartesian(xlim=xRange,ylim =c(-5,5))
  } else {
    cov_xx_inv <- solve(cov(obs$x, obs$x,l,sigma) + noise^2 * diag(1, length(obs$x)))
    Ef <- cov(x_predict, obs$x,l,sigma) %*% cov_xx_inv %*% obs$y
    Cf <- cov(x_predict, x_predict,l,sigma) - cov(x_predict, obs$x,l,sigma)  %*% cov_xx_inv %*% cov(obs$x, x_predict,l,sigma)
    # predictions in data frame
    value <- mvrnorm(5, Ef, Cf)
    dat <- data.frame(x=x_predict, t(value))
    dat <- melt(dat, id="x")
    # Actual plotting bits:
    fakeData = data.frame(x=x_predict,y=Ef,lower=(Ef-2*sigma*sqrt(diag(Cf))),upper=(Ef+2*sigma*sqrt(diag(Cf))))
    ggplot(dat,aes(x=x,y=value)) +
      geom_ribbon(data=fakeData, aes(y=y,ymin=lower, ymax=upper), fill="grey80") +
      geom_line(aes(group=variable,color=variable)) + #REPLICATES
      geom_line(data=fakeData,aes(x=x,y=y), size=1) + #MEAN
      geom_point(data=obs,aes(x=x,y=y),size=2,shape=21) +  #OBSERVED DATA
      ylab("output, f(x)") + xlab("input, x") +
      coord_cartesian(xlim=xRange,ylim =c(-5,5)) +
      theme_bw()+theme(legend.position="none")
  }
}

```

## A quick introduction to Gaussian Process models

- Motivation
- Going from parametric to non-parametric models
- Gaussian Processes
- Kernel Parameters
- Demo
- Generalizations

## Motivation

Given observations data generated by some unknown function, we'd like to make predictions for new values (the usual regression problem). Some limitations of other approaches include:

- Parametric approaches require choosing an appropriate functional form
- Neural network-type approaches don't always provide well-calibrated uncertainty estimates rather than just point estimates (although getting better?)
- Sometimes you just don't have a ton of data to fit something like a neural network

Gaussian processes provide a flexible non-parametric approach to approximating an unknown function which largely addresses the above issues (while of course introducing fun new complications)


## Parametric Models

Strategy: 

- Model the data as the outputs of some parametric function, turning the learning problem into finding the parameters that best parameters given the data
    * Best here meaning maximizing some objective function or minimizing some loss function
    * Alternatively can consider the posterior distribution $p(\theta | \mathcal{D})$ over parameters in the case of Bayesian inference.
- Typically comparatively efficient to because functions have been selected to be easy to work with

## Linear models

Due to ease of training/the fact that they're pretty reasonable predictive models despite making an almost always bad assumption about functional form, linear models are a common choice. These take the form

$$ f(x) = x^Tw $$

where the weight vector $w$ is the parameter to be learned. Observations are then

$$y = x^Tw + \epsilon$$ 

where $\epsilon$ is $\mathcal{N}(0,\sigma)$ noise).

## Linear fit

```{r,echo=FALSE}

x <- runif(10,-5,5)
y <- 3*x-2 + rnorm(10,0,2)

ggplot(data.frame(x=x,y=y),aes(x=x,y=y)) +
  geom_point()+geom_smooth(method = "lm",se=TRUE)+
  xlab("x") + ylab("f(x)")+
  theme_bw()

```

## Basis Expansion

Sometimes a linear function of the input isn't ideal. In this case, we can model non-linear functions of the data using basis expansion. This takes the form

$$ f(x) = \phi(x)^Tw$$

where $\phi(x)$ is a function projecting $x$ into some higher-dimensional feature space. While it's a non-linear function of the data, is linear in the parameters and so preserves the computational advantages of linear models.

##
Some examples include:

- Polynomial regression -- features are $x$, $x^2$,... $x^n$
- Periodic basis functions -- features are periodic functions like $sin(x)$, $cos(x)$ with varying frequency and phase
- Radial Basis Functions -- features are the similarity of the input to some fixed set of points $c$ given by $$exp\left(-\frac{(x-c_i)^2}{2\sigma^2}\right)$$ where $\sigma$ is the bandwidth of the function (kernel density estimators use something similar).

## Polynomial Regression

Regression with $\phi(x)= [x^0, x, x^2, x^3]$

```{r,echo=FALSE}

x <- runif(10,-5,5)
y <- -2+x-5*x^2+0.5*x^3 + rnorm(10,0,2)

ggplot(data.frame(x=x,y=y),aes(x=x,y=y)) +
  geom_point()+geom_smooth(method = "lm",formula = y~poly(x,2),se=TRUE)+
  xlab("x") + ylab("f(x)")+
  theme_bw()

```


## Periodic Basis Functions

$\phi(x) = [1, sin(\pi x),cos(10\pi x)]$ (cheating on choices)

```{r,echo=FALSE}

x <- runif(500,-5,5)
y <- sin(pi*x) + 0.1*cos(10*pi*x) + rnorm(10,0,0.25)

ggplot(data.frame(x=x,y=y),aes(x=x,y=y)) +
  geom_point()+ geom_smooth(method = "lm",formula = y~sin(pi*x)+cos(10*pi*x), se=TRUE)+
  xlab("x") + ylab("f(x)")+
  theme_bw()

```

##

Could just as easily miss the smaller-magnitude component (and the fit previously wasn't so great)

```{r,echo=FALSE}

x <- runif(500,-5,5)
y <- sin(pi*x) + 0.1*cos(10*pi*x) + rnorm(10,0,0.25)

ggplot(data.frame(x=x,y=y),aes(x=x,y=y)) +
  geom_point()+ geom_smooth(method = "lm",formula = y~sin(pi*x), se=TRUE)+
  xlab("x") + ylab("f(x)")+
  theme_bw()

```


## Radial Basis Functions

model with three rbfs at -2, 0, and 2 (all with bandwidth 1)

```{r,echo=FALSE}

x <- runif(50,-5,5)
y <- -2+x-5*x^2+0.5*x^3 + rnorm(10,0,2)

rbf <- function(x,c,s) {exp(-(x-c)^2/(2*s^2))}

ggplot(data.frame(x=x,y=y),aes(x=x,y=y)) +
  geom_point()+ geom_smooth(method = "lm",formula = y~rbf(x,-2,1)+rbf(x,0,1)+rbf(x,2,1), se=TRUE)+
  xlab("x") + ylab("f(x)")+
  theme_bw()

```

## Adjusting rbf parameters

rbfs at -3, -2, -1, 0, 1, 2, 3, all with bandwidth 0.5

```{r,echo=FALSE}

x <- runif(50,-5,5)
y <- -2+x-5*x^2+0.5*x^3 + rnorm(10,0,2)

rbf <- function(x,c,s) {exp(-(x-c)^2/(2*s^2))}

ggplot(data.frame(x=x,y=y),aes(x=x,y=y)) +
  geom_point()+ geom_smooth(method = "lm",formula =
                              y~rbf(x,-2,0.5)+ rbf(x,0,0.5)+ rbf(x,2,0.5)+ rbf(x,-1,0.5)+ rbf(x,1,0.5)+ rbf(x,-3,0.5) +rbf(x,3,0.5),
                            se=TRUE)+
  xlab("x") + ylab("f(x)")+
  theme_bw()

```


## Gaussian Processes

As an alternative to considering finite sets of parametric basis functions, another option is to instead consider a probability distribution over the space of functions. Gaussian Processes are one approach to this.

```{r,echo=FALSE,fig.height=3}
plotGP(xRange = c(-5,5),l=1,sigma=1)+ggtitle("Samples from a Gaussian Process prior")
```

##

These are

- An infinite-dimensional generalization of the Gaussian distrubution, parameterized by a mean and covariance *functions* $m$ and $C$ (where $C_{ij}=k(x_i,x_j))$ for some kernel $k$), giving the distribution $p(f|X) \sim \mathcal{N}(m,C)$

- Valid covariance functions are limited to ones that result in a positive definite covariance matrix.

- For any finite collection of points, this is a gaussian distribution.

So, taking a GP prior and conditioning on data gives us a GP posterior over functions given our data, constraining the space of possible functions to those compatible with our observations.

## Defining Kernel Functions

Any kernel function that results in a positive definite covariance matrix can be used, but finding useful ones seems to be a matter of experimentation. Some commonly used kernels include

- Squared exponential (this looks like a gaussian distribution)
- Rational Quadratic (equivalent to a sum of SE kernels with different length-scales)
- Matern (gives functions which are not infinitely differentiable)
- Periodic
- Locally Periodic (periodicity decays with distance)

Kernel functions can also be build by multiplying or adding together simpler functions to build up more complex functions.

##

MacKay (2003), ch. 45 also gives a method of defining valid kernels in terms of frequency response:

- Define some positive function of frequency (specifying kernel in terms of frequency response)
- Define kernel function as the inverse Fourier transform of the frequency response


## Squared Exponential Kernel
The examples below use a squared exponential kernel with $m=0$

$$k(x,x') = \sigma^2*exp\left[ -0.5*\frac{(x-x')^2}{l^2}\right]$$

This has the (hyper-) parameters $l$ which controls the length-scale (how quickly similarity between points falls off), and and $\sigma$ which determines the overall scale of changes in the function.

## Conditioning on observations/making new predictions:

Starting from the joint distribution of observations and points to be predicted:

$$\begin{bmatrix} f(x_{obs}) \\ f(x_{pred}) \end{bmatrix} \sim \mathcal{N}\left(0,\begin{bmatrix} K(x_{obs},x_{obs}) K(x_{obs},x_{pred}) \\ K(x_{pred},x_{obs}),K(x_{pred},x_{pred})\end{bmatrix}\right)$$


##

The conditional distribution $p(f(x_{pred})|x_{pred},x_{obs},f(x_{obs}))$ becomes

$$p(f(x)_{post}|x_{pred},x_{obs},f(x_{obs})) = \mathcal{N}(m(x)_{post} ,C(x)_{post})$$

where

$$m(x)_{post} = m(x_{pred}) + K_{obs,pred}^TK_{obs,obs}^{-1}(f(x_{obs})-m(x_{obs}))$$

and

$$C(x)_{post} = K_{pred,pred} - K_{obs,pred}^TK_{obs,obs}^{-1}K_{obs,pred}$$


Unfortunately, since this involves a matrix inversion, computational cost scales as $\mathcal{O}(N^3)$. However, there are faster approximations available.

## 1-dimensional example:
```{r, echo=FALSE, eval=TRUE}

inputPanel(
    sliderInput("l", label="length-scale:", min=0.1, max=5, value=1, step=0.1),
    selectInput("obs",label="observations", choices = c(FALSE,TRUE)),
    sliderInput("sigma",label="sigma",min=0.1,max=3,step=0.1,value=1)
)

renderPlot({
  if(input$obs){
    observations = data.frame(x = c(-4, -3, -1,  0,  2), y = c(-2,  0,  1,  2, 3))
    plotGP(xRange = c(-5,5),obs=observations,l=input$l,sigma=input$sigma)
  } else {
    plotGP(xRange = c(-5,5),l=input$l,sigma=input$sigma)
  }
})
```

## Measuring with noise

The previous example assumed we were measuring with no noise (so the function has to pass through the observations). This isn't usually the case (and it can be nice to have a less constrained function) so noise can be added to the model (this is another tuneable hyperparameter). Form for predictions remains the same.

$$k(x,x') = \sigma^2_f*exp\left[ -0.5*\frac{(x-x')^2}{l^2}\right] + \sigma^2_y\delta_{xx'}$$


## 1-dimensional example with noise

```{r, echo=FALSE, eval=TRUE}

inputPanel(
    sliderInput("l2", label="length-scale:", min=0.1, max=5, value=1, step=0.1),
    sliderInput("sigma2",label="sigma",min=0.1,max=3,step=0.1,value=1),
    sliderInput("noise",label="noise",min=0,max=3,value=0,step=0.1)
)

renderPlot({
    observations = data.frame(x = c(-4, -3, -1,  0,  2), y = c(-2,  0,  1,  2, 3))
    plotGP(xRange = c(-5,5),obs=observations,l=input$l2,sigma=input$sigma2,noise=input$noise)
    })
```


## More complex input:

Even with $m=0$, the model is sufficiently flexible to capture more complicated functions (here a cosine wave)

```{r,echo=FALSE}
x <- runif(500,-5,5)
y <- 2*cos(2*pi*x)+rnorm(500,0,0.1)
obs <- data.frame(x=x,y=y)
plotGP(xRange = c(-5,5),obs=obs,l=0.25,sigma=2,noise=1)


```

##
### 2-dimensional models:

These models can be extended to deal with multidimensional input as well. In this case, the kernel becomes

$$k(x,x') = \sigma^2_f*exp\left[ -0.5*\frac{(x-x')^TL^{-1}(x-x')}{l^2}\right]+ \sigma^2_y\delta_{xx'}$$

where $L$ is a matrix specifying the length scale. If $L$ is diagonal with all $L_{ii} = l^2$, this corresponds to having the same $l$ for all dimensions. If it's diagonal but without tied values, this allows length scales to vary between dimensions.


## Non-isotropic kernels

Non-isotropic kernels have a few applications:

- Sometimes different input dimensions simply vary greatly in scaling and it's useful to account for this
- When learning kernel parameters from data, having different values $l_d$ for each input dimension, we get a gaussian process where the relavance of each dimension to the prediction can be varied independently (e.g. for a dimension with very high $l_d$, small variations are effectively irrelevant).
    * Often called an Automatic Relevance Determination (ARD) kernel

## Learning Kernel Parameters
Choice of kernel parameters is pretty important, so generally we want to learn these from data. Approaches include:

- Map and Maximum Likelihood estimation (potential issues with local maxima)
- Markov Chain Monte Carlo (MCMC) (slower)
- Variational approximation (some issues with underestimating posterior variance)


## Scalable inference

Exact inference for GPs can be computationally expensive or effectively intractable for sufficiently large data sets/more complex models. A number of methods for more efficient inference exist.

- Typically based on the approach of finding a good set of basis points, scaling with the size of this set rather than the size of the data.
- Online learning methods also exist (these necessarily use something like the sparse approach as well)


## Extensions:

While the examples here had real-valued inputs, this isn't a requirement. Kernels of various sorts exist for structured inputs like images, strings, etc. There are also a number of extensions of GPS for other purposes:

- GP-LVMs: PCA-like unsupervised learning of some latent variable projection of the data
- Classification and count data models: These have roughly the same relationship as GLMs have to linear models, although they introduce some new complications to inference due to non-gaussian likelihood
- Deep GPs: A deep learning approach which uses stacks of multiple GL-LVMs. This potentially performs well in comparatively sparse-data circumstances.
